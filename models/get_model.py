# -*- coding: utf-8 -*-
"""FINAL-PRO_AI[master].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5aDKg2xLeER4v2aSwfcuuOlSFtUvp6v
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import joblib

# Load the dataset
try:
    data = pd.read_csv("Dataset of Diabetes .csv")
except FileNotFoundError:
    import kagglehub
    path = kagglehub.dataset_download("aravindpcoder/diabetes-dataset")
    data = pd.read_csv(path + "/Dataset of Diabetes .csv")
    data.to_csv('Dataset of Diabetes (ASLI).csv', index=False)

# Clean and preprocess data
data["CLASS"] = data["CLASS"].replace("Y", "Positive")
data["CLASS"] = data["CLASS"].replace("P", "Possible")
data["CLASS"] = data["CLASS"].replace("N", "Negative")
data["CLASS"] = data["CLASS"].replace("Y ", "Positive")
data["CLASS"] = data["CLASS"].replace("N ", "Negative")

# Drop rows where CLASS is 'Possible'
data = data[data["CLASS"] != "Possible"]

# Clean gender data
data["Gender"] = data["Gender"].replace("f", "F")

# Remove duplicates based on ID and No_Pation
duplicates = data[data.duplicated(subset=['ID', 'No_Pation'], keep=False)]
duplicate_ids = duplicates[['ID', 'No_Pation']].drop_duplicates()
for _, row in duplicate_ids.iterrows():
    id_val = row['ID']
    pation_val = row['No_Pation']
    duplicate_rows = data[(data['ID'] == id_val) & (data['No_Pation'] == pation_val)]
    if len(duplicate_rows)>1:
        data.drop(duplicate_rows.index[1:], inplace=True)

# Drop unnecessary columns
data = data.drop(['ID', 'No_Pation', 'HDL'], axis=1)

# Prepare data for training
X = data.drop('CLASS', axis=1)
y = data['CLASS']

# Encode categorical features
encoder = LabelEncoder()
X['Gender'] = encoder.fit_transform(X['Gender'])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Create resampled DataFrame
resampled_data = pd.DataFrame(X_train_resampled, columns=X.columns)
resampled_data['CLASS'] = y_train_resampled

# Save resampled data
resampled_data.to_csv('resampled_data.csv', index=False)

# Train Random Forest model with best parameters
rf_classifier = RandomForestClassifier(
    random_state=42,
    n_estimators=100,
    criterion='gini',
    max_depth=9,
    max_features=10,
    min_samples_split=4,
    min_samples_leaf=2
)

# Train model
rf_classifier.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred_best = rf_classifier.predict(X_test)

# Calculate metrics with proper label handling
tuned_accuracy = accuracy_score(y_test, y_pred_best) * 100
tuned_precision = precision_score(y_test, y_pred_best, pos_label='Positive') * 100
tuned_recall = recall_score(y_test, y_pred_best, pos_label='Positive') * 100
tuned_f1 = f1_score(y_test, y_pred_best, pos_label='Positive') * 100

# Print performance metrics
print(f"\nPerformance with Tuned Parameters:")
print(f"Akurasi model Random Forest: {tuned_accuracy:.2f}")
print(f"Precision model Random Forest: {tuned_precision:.2f}")
print(f"Recall model Random Forest: {tuned_recall:.2f}")
print(f"F1 Score model Random Forest: {tuned_f1:.2f}")

print(f"\nClassification Report with Tuned Parameters:\n")
print(classification_report(y_test, y_pred_best))

# Save the model
joblib.dump(rf_classifier, 'random_forest_model_tuned_params.joblib')
print("\nTuned Random Forest model saved as 'random_forest_model_tuned_params.joblib'")

# Save min-max values for future use
min_max_values = {}
for col in resampled_data.columns[:-1]:
    min_max_values[col] = (resampled_data[col].min(), resampled_data[col].max())
min_max_df = pd.DataFrame(min_max_values, index=['Min', 'Max']).T
min_max_df.to_csv('min_max.csv', index=False)